{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA2\n",
    "\n",
    "## Intro to Convolutional Neural Nets with Keras\n",
    "\n",
    "Contest on Cat/Dog classification. \n",
    "\n",
    "Due T, 6/14/2022, 5pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contest\n",
    "\n",
    "Besides the ~20k train/test images that we load below, there are ~5,000 more unseen/hidden cat/dog images in a private folder. \n",
    "\n",
    "Train a CNN model of your own design, tune it, (cross-validate it if you want!) until you are satisfied with its performance.\n",
    "\n",
    "I will run your saved models on this competition dataset and let you know how well your model fares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part: You must **create your own model** such as the VGG example or a smaller one of your own design! \n",
    "\n",
    "\n",
    "### Important: Your  model has to be named `comp_model` \n",
    "\n",
    "``` python\n",
    "comp_model = myNN(X_train)\n",
    "```\n",
    "\n",
    "If your model name is incorrect, my call will fail and so may you! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2d71c342e4624dfa3a1df4a65c6b8644",
     "grade": false,
     "grade_id": "cell-20a94lkm51bca15f1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## What can I use?\n",
    "- Your code can use feature engineering, normalization or other common tricks\n",
    "- You can use Dense, Conv2D or similar layers in Keras \n",
    "- You can use dropout, normalization, etc.\n",
    "- You can (and should!) plot your history and observe your model's behaviour. \n",
    "- You can choose your favorite 'optimizer' or 'loss' functions.\n",
    "- Your 'metrics' HAS to be:  `metrics = ['accuracy','mae']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "289631525b99ad119f1572bb6d02b0df",
     "grade": false,
     "grade_id": "cell-20a9jik9m51bca15f1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Comments\n",
    "- **Results**: Validation alone is not enough for this assignment as some parts are manually graded. For me to able to view your results, you need to ensure that your notebook runs. One way to test is to click: `Kernel`->`Restart & Run All` and verify that all cells execute without errors. \n",
    "- **Runtime**: Each notebook has a runtime limit. Your submitted notebook should execute in under a min. Thus, if you are using iterative algos, similar to scikit's GridSearch, comment them out before submitting your notebook. For instance, you can hard code the best coefs returned by such algorithms and comment out the search to save execution time.\n",
    "- **Randomness**: if you are using an algo that depends on random state, make sure it is reproduceable. For instance, set the random seed so that your model behaves similary when I execute it.  You could also [save your model and load it](https://www.tensorflow.org/guide/keras/save_and_serialize) to get around this. \n",
    "- Above also means that you can comment out your training code to save execution time and avoid the randomness issue. MAke sure to load your model as 'comp_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "\n",
    "- Name your trained Keras model as `comp_model` \n",
    "- Your 'metrics' HAS to be:  `metrics = ['accuracy','mae']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train/test data:\n",
    "import numpy as np\n",
    "path = '/home/memo/public/eaix/'\n",
    "\n",
    "X_train = np.load(path+'X_train.npy')\n",
    "X_test = np.load(path+'X_test.npy')\n",
    "y_train = np.load(path+'y_train.npy')\n",
    "y_test = np.load(path+'y_test.npy')\n",
    "print(X_train.shape, X_test.shape) #should be (14967, 50, 50, 1) (4990, 50, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "09ee51a665c4b087017f280cc56fd213",
     "grade": true,
     "grade_id": "cell-5d8459d3a6d6ba5d",
     "locked": false,
     "points": 50,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "85aadb3af7dfe77acf439f3dd3c7a67d",
     "grade": true,
     "grade_id": "cell-0fd7a62ff7c436de",
     "locked": false,
     "points": 50,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history: Sample code\n",
    "#ne is number of epochs to plot. Update it! \n",
    "ne = 1\n",
    "# History object is a dictionary with keys. \n",
    "hd = history.history\n",
    "\n",
    "loss_tr = hd['accuracy']\n",
    "loss_va = hd['val_accuracy']\n",
    "epochs = range(0, ne) #ne is number of epochs. Set it! \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "plt.plot(epochs, loss_tr, '-.o', label='Training Acc')\n",
    "plt.plot(epochs, loss_va, 'r', label='Validation Acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap it up\n",
    "\n",
    "Once you are happy with your model, you can comment out your training code (model.fit() ) and SAVE your model: \n",
    "\n",
    "\n",
    "How to save/load your model: https://www.tensorflow.org/guide/keras/save_and_serialize\n",
    "\n",
    "Pseudocode to save/load and test a Keras NN name 'mymodel': \n",
    "``` python\n",
    "model.save('mymodel') #save your model\n",
    "comp_model = load_model('mymodel') #load it as desired name\n",
    "#Comment out: model.fit, history and model.save() if load works.\n",
    "#Evaluate loaded models performance on test data:\n",
    "nnmse, nnacc, nnmae = comp_model.evaluate(X_test, y_test, verbose = 1)\n",
    "print('*** Test *** ')\n",
    "print('NN Test MAE: ', nnmae)\n",
    "print('NN Test ACC: ', nnacc)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "72ec5986edf28571d9e5511aba2cdb74",
     "grade": false,
     "grade_id": "cell-0399b84769f5fb0e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Performance on hidden data\n",
    "\n",
    "You shouldn't be able to run the following cells as these are hidden files. But, I'll run them to evaluate your models performance on this data. \n",
    "\n",
    "Your model must be saved as `comp_model` for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1a5ed3c61ad251cd3e911993a285dc2a",
     "grade": false,
     "grade_id": "cell-4916d7a141213f7e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#read hidden data\n",
    "#Note this data is NOT visible to you. \n",
    "X_val =  np.load('/home/memo/private/X_val.npy')\n",
    "y_val =  np.load('/home/memo/private/y_val.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "28454e89d05c50a3eea8a35183ec51ac",
     "grade": false,
     "grade_id": "cell-a95f9354ff69cf7a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Competition data. You can't run this.\n",
    "nnloss, nnacc, nnmae = comp_model.evaluate(X_val, y_val, verbose = 1)\n",
    "#final score on hidden dataset:\n",
    "print(\"Competition accuracy is %.2f\" % (nnacc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
