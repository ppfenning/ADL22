{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk7TJEJMaXMs"
   },
   "source": [
    "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
    "\n",
    "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
    "\n",
    "This notebook was generated for TensorFlow 2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E19PxAejMmEp"
   },
   "source": [
    "Based on Chollet's notebooks:\n",
    "- chapter04_getting_started_with_neural_networks\n",
    "- chapter05_Dropout\n",
    "- Keras API documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDlLYLiGaXMu"
   },
   "source": [
    "# Getting started with neural networks: Classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Text preprocessing\n",
    "    - Encoding/Decoding data to work with a neural network\n",
    "\n",
    "- Monitoring for overfitting\n",
    "    - Number of epochs to train to avoid Validation testing performance loss\n",
    "\n",
    "- Deeper dive into training\n",
    "    - Early stopping and other callbacks\n",
    "    \n",
    "- Improve Model Generalization\n",
    "    - Regularization / Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I2pUD5VaXMv"
   },
   "source": [
    "## Classifying movie reviews: \n",
    "\n",
    "A binary classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca9MsTXXaXMv"
   },
   "source": [
    "### The IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SuFSe1RaXMw"
   },
   "source": [
    "**Loading the IMDB dataset**\n",
    "\n",
    "A set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews.\n",
    "\n",
    "the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary. This enables us to focus on model building, training, and evaluation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "X-xAOx2qaXMw"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "words = 10000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=words)\n",
    "# only keep the top 3,000 most frequently occurring words in the training data. (out of ~90k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUhzr_9Cr_Gi",
    "outputId": "b5505bff-b811-4594-fa61-c1cfde3241ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ny_3Rqh1Wdx2"
   },
   "source": [
    "The variables train_data and test_data are lists of reviews; each review is a list of word indices (encoding a sequence of words). \n",
    "\n",
    "train_labels and test_labels are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwNHFlJsaXMx",
    "outputId": "dd5ca03d-e2ec-4bb3-929e-6dbe31a9a586"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][:11] #coded first 11 words of the Review #0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJC359JfaXMx",
    "outputId": "31c716f1-b9f9-4d79-91a0-fef324959fc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z45q1DrCaXMy",
    "outputId": "5c703377-6e0e-4373-ae7f-6e30037d2f4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "2999"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data]) #top 3k words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcA7SmQlaXMy"
   },
   "source": [
    "**Decoding reviews back to text**\n",
    "\n",
    "Words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data.\n",
    "\n",
    "We can go back and forth between \"3\" and associated word or vice versa! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vecKfBLcaXMy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "WOjoabaSY_Ub",
    "outputId": "1e87c23a-118a-4c8d-8b48-1aad861b875b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"? this film was just brilliant casting location scenery story direction ? really ? the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same ? island as myself so i loved the fact there was a real connection with this film the witty ? throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly ? was amazing really ? at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little ? that played the ? of ? and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don't you think the whole story was so lovely because it was true and was ? life after all that was ? with us all\""
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_review = \" \".join(\n",
    "    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])\n",
    "\n",
    "\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80tyVJ7AQTAD",
    "outputId": "d207a092-e827-4940-a3ca-ac55801b73fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SeABDaTLYiUA",
    "outputId": "f856c567-4005-44d5-cd73-43f3e17dcadf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ?\n",
      "14 this\n",
      "22 film\n",
      "16 was\n",
      "43 just\n",
      "530 brilliant\n",
      "973 casting\n",
      "1622 location\n",
      "1385 scenery\n",
      "65 story\n",
      "458 direction\n"
     ]
    }
   ],
   "source": [
    "#decode integers back to words\n",
    "for i in train_data[0][:11]:\n",
    "    print(i, reverse_word_index.get(i-3, \"?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQZ72xv4aXMz"
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nInsbqJgaXMz"
   },
   "source": [
    "**Encoding the integer sequences via multi-hot encoding**\n",
    "\n",
    "Each review is different length. Format is no go as is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "_roXIA4zYWpw",
    "outputId": "476d2b8e-9542-4a4f-925d-e1ff83e3a597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data[0]: 218\n",
      "train_data[33]: 162\n",
      "train_data[333]: 114\n"
     ]
    }
   ],
   "source": [
    "#Note that length of reviews vary \n",
    "print(\"train_data[0]:\", len(train_data[0]))\n",
    "print(\"train_data[33]:\", len(train_data[33]))\n",
    "print(\"train_data[333]:\", len(train_data[333]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3D7AaGRIGPj"
   },
   "source": [
    "**Multi-hot encode** your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [8, 5] into a 10,000-dimensional vector that would be all 0s except for indices 8 and 5, which would be 1s. Then you could use a Dense layer, capable of handling floating-point vector data, as the first layer in your model. \n",
    "\n",
    "Note that our train data size is now increased significantly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "N3rVjWmOaXMz"
   },
   "outputs": [],
   "source": [
    "# Multi-hot encode your lists to turn them into vectors of 0s and 1s\n",
    "import numpy as np\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1g2-7ArXp2F",
    "outputId": "87eb01f7-02cf-4778-854d-a8b49f9a3157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: (25000,) \n",
      "x_train: (25000, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data:\", train_data.shape, \"\\nx_train:\", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9XuP1BsaXMz",
    "outputId": "ef9dd744-c550-4113-d6c1-a8c19d6ae1ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ouvMUPETaXM0"
   },
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0gBbMR5aXM0"
   },
   "source": [
    "### Building your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-RmyT2xaXM0"
   },
   "source": [
    "**Model definition**\n",
    "\n",
    "What type of problem are we solving? \n",
    "\n",
    "(Binary) classification.\n",
    "\n",
    "Desnse layers with ReLu activation is a good start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "il2mIk06aXM0"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\") #WHY 1 NEURON?\n",
    "    #Why are we using a sigmoid?\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1CgE3YaxpRW"
   },
   "source": [
    "## Activations:\n",
    "\n",
    "### Sigmoid:\n",
    "A sigmoid “squashes” arbitrary values into the [0, 1] interval (see figure 4.3), outputting something that can be interpreted as a probability.\n",
    "\n",
    "<img src=https://drek4537l1klr.cloudfront.net/chollet2/Figures/04-03.png>\n",
    "\n",
    "### ReLu:\n",
    "(rectified linear unit) is a function meant to zero out negative values\n",
    "\n",
    "<img src=https://drek4537l1klr.cloudfront.net/chollet2/Figures/04-02.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pczfk0LaXM0"
   },
   "source": [
    "**Compiling the model**\n",
    "\n",
    "Dealing with a binary classification problem and the output of your model is a probability (you end your model with a single-unit layer with a sigmoid activation), it’s best to use the binary_crossentropy loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tWM3WtWGaXM1"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBSygD-DaXM1"
   },
   "source": [
    "### Validating your approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU0oUeUQaXM1"
   },
   "source": [
    "**Setting aside a validation set**\n",
    "\n",
    "create a validation set by setting apart 10,000 samples from the original training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xKn5t_KxaXM1"
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbTZM6yOaXM1"
   },
   "source": [
    "**Training your model**\n",
    "\n",
    "Save traingin **history** while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHxjGVt5aXM1",
    "outputId": "e6e494be-7c96-498e-d12a-709e35af9f64"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-90a50ee7d258>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m history = model.fit(partial_x_train,\n\u001B[0m\u001B[1;32m      2\u001B[0m                     \u001B[0mpartial_y_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m                     \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m                     \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m512\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                     validation_data=(x_val, y_val))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEJ92OpQaC7I"
   },
   "source": [
    "The *history* dictionary contains four entries: one per metric that was being monitored during training and during validation. \n",
    "\n",
    "In the following two listings, let’s use Matplotlib to plot the training and validation loss side by side (see figure 4.4), as well as the training and validation accuracy (see figure 4.5). \n",
    "\n",
    "Note that your own results may vary slightly due to a different random initialization of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1U0MmSWaXM2",
    "outputId": "11a88de0-14bb-4f38-984a-6b0407da6e82"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3zXGpc_prB2",
    "outputId": "7096f9a3-7996-43f3-ca60-0ba0880031ab"
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Exg0sDXHaXM2"
   },
   "source": [
    "**Plotting the training and validation loss**\n",
    "\n",
    "Importance of validating your model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "cMvs9nwCaXM2",
    "outputId": "9eb9c209-3186-43d4-c910-bd7a12504631"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\") \n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwitwaydaXM2"
   },
   "source": [
    "**Plotting the training and validation accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "wzqSMyfWaXM2",
    "outputId": "6dafaf59-7fae-4175-8a9c-afc50edc7d78"
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring for overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoR7eITKaXpU"
   },
   "source": [
    "As you can see, the training loss decreases with every epoch, and the training accuracy increases with every epoch. That’s what you would expect when running gradient-descent optimization—the quantity you’re trying to minimize should be less with every iteration. \n",
    "\n",
    "But that isn’t the case for the validation loss and accuracy: they seem to peak at the ***?dort?*** epoch. This is an example of what we warned against earlier: a model that performs better on the training data isn’t necessarily a model that will do better on data it has never seen before. \n",
    "\n",
    "In precise terms, what you’re seeing is **overfitting**: after the *???* epoch, you’re overoptimizing on the training data, and you end up learning representations that are specific to the training data and don’t generalize to data outside of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6Zu6uLvaXM2"
   },
   "source": [
    "**Retraining a model from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "YeSJoxCIaXM2",
    "outputId": "20006c60-8b3a-4801-9c6b-eafb2bb657f2"
   },
   "outputs": [],
   "source": [
    "e = 1 #number of epochs to avoid overtraining\n",
    "# update!!\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, epochs=e, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFU0qizgaXM2",
    "outputId": "387552d4-dec1-4e37-dea1-eb5c5f11e721"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2v7RnF19a6NJ"
   },
   "source": [
    "This fairly naive approach achieves an accuracy of 88%. \n",
    "\n",
    "With state-of-the-art approaches, you should be able to get close to 95%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faK5yWtSaXM3"
   },
   "source": [
    "### Using a trained model to generate predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zsS6bWuhaXM3",
    "outputId": "7eec15d3-bd55-4f4c-de33-bcf15df29ea1"
   },
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TOEEtXkd3DLL",
    "outputId": "b8dd4915-e2f6-4856-bcff-c1a0b51af3f5"
   },
   "outputs": [],
   "source": [
    "#FUN!\n",
    "#Test your  models performance on your own reviews! \n",
    "\n",
    "def return_review_pred(r, model):\n",
    "    l = [] #list of each word in r\n",
    "    for word in r.split(\" \"):\n",
    "    # print(word)\n",
    "        l.append(word_index.get(word)+3)\n",
    "    l = np.vstack([l,l]) #duct tape\n",
    "  #encode each word to an int and Multi-hot\n",
    "    x_r = vectorize_sequences(l)\n",
    "  # print(x_r.shape) #how long does each sentance become?\n",
    "    return model.predict(x_r) #What is 0 vs 1?\n",
    "\n",
    "#Code below should take your review in r and encoded to be analyzed by the model! \n",
    "r = \"love this film\"\n",
    "return_review_pred(r, model)[0]\n",
    "#what does 0.6 mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvoUEIaCaXM3"
   },
   "source": [
    "### Further experiments\n",
    "\n",
    "*   You used two representation layers before the final classification layer. Try using one or three representation layers, and see how doing so affects validation and test accuracy.\n",
    "*   Try using layers with more units or fewer units: 32 units, 64 units, and so on.\n",
    "*   Try using the mse loss function instead of binary_crossentropy.\n",
    "*   Try using the tanh activation (an activation that was popular in the early days of neural networks) instead of relu. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper dive into training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what to look for in our train vs validate plots, we can be smarter about it. \n",
    "\n",
    "Goal:\n",
    "- Train as long as it avoids overfitting\n",
    "- No formula\n",
    "- How to automate?\n",
    "    - Monitor Loss on Validation data and see when it stops improving. \n",
    "\n",
    "\n",
    "Since we can save the state of our model, we can code this! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool: *callback*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional parameter passed to model.fit(_)\n",
    "Has access to data and state of model\n",
    "Can:\n",
    "- Save model at different stages during training\n",
    "- Interrupt training\n",
    "- Alter model state, including loading diff weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available callbacks:\n",
    "https://keras.io/api/callbacks/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common callbacks use cases, from Chollet Ch. 7.3.2:\n",
    "\n",
    "- **Model checkpointing**—Saving the current state of the model at different points during training. \n",
    "- **Early stopping**—Interrupting training when the validation loss is no longer improving (and of course, saving the best model obtained during training). \n",
    "- **Dynamically adjusting** the value of certain parameters during training—Such as the learning rate of the optimizer. \n",
    "- **Logging** training and validation metrics during training, or visualizing the representations learned by the model as they’re updated—The fit() progress bar that you’re familiar with is in fact a callback!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use callback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Live coding to fill in the blanks. \n",
    "\n",
    "\n",
    "# Callbacks are passed to the model via the callbacks argument in fit(), \n",
    "# which takes a list of callbacks. \n",
    "# You can pass any number of callbacks\n",
    "callbacks_list = [\n",
    "    #Interrupts training when improvement stops\n",
    "    keras.callbacks.EarlyStopping( \n",
    "        #Monitors the model’s validation accuracy\n",
    "\n",
    "        #Interrupts training when accuracy has stopped improving for N epochs\n",
    "    ),\n",
    "    #Saves the current weights after every epoch\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        #Path to the destination model file\n",
    "\n",
    "        #These two arguments mean you won’t overwrite the model file unless val_loss has improved, \n",
    "#         which allows you to keep the best model seen during training. \n",
    "\n",
    ")\n",
    "    #We used model.save() before which saves only AFTER training, not during. \n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load callback0.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update your training to include your callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old:\n",
    "# model.fit(x_train, y_train, epochs=e, batch_size=512)\n",
    "\n",
    "#New:\n",
    "#fill with callbacks:\n",
    "# model.fit( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Model Generalization\n",
    "\n",
    "- perform better in unseen data\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actively *impede* the model’s ability to fit perfectly to the training data, with the goal of making the model perform better during validation. \n",
    "This is called “regularizing” the model, because it tends to make the model simpler, more “regular,” its curve smoother, more “generic”; thus it is less specific to the training set and better able to generalize by more closely approximating the latent manifold of the data.\n",
    "\n",
    "Many options, but worthy to mention: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randomly dropping out (setting to zero) a number of output features of the layer during training.\n",
    "\n",
    "Controlled by a parameter: **dropout rate**\n",
    "\n",
    "The dropout rate is the fraction of the features that are zeroed out; it’s usually set between 0.2 and 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Add after each layer:\n",
    " \n",
    "   `layers.Dropout(0.5)`\n",
    "   \n",
    "Good for Dense, but may not be for Conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5), #50%: we set half of the output randomly to 0 during training\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation=\"sigmoid\") #WHY 1 NEURON???\n",
    "    #A new output activation! What did we use last time? Why are we using a sigmoid?\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, \n",
    "          epochs=20, #lets go crazy\n",
    "#           callbacks=callbacks_list,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting the training and validation loss**\n",
    "\n",
    "Importance of validating your model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\") \n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting the training and validation accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the training loss decreases with every epoch, and the training accuracy increases with every epoch. That’s what you would expect when running gradient-descent optimization—the quantity you’re trying to minimize should be less with every iteration. \n",
    "\n",
    "But that isn’t the case for the validation loss and accuracy: they seem to peak at the ??? epoch. This is an example of what we warned against earlier: a model that performs better on the training data isn’t necessarily a model that will do better on data it has never seen before. \n",
    "\n",
    "In precise terms, what you’re seeing is **overfitting**: after the ??? epoch, you’re overoptimizing on the training data, and you end up learning representations that are specific to the training data and don’t generalize to data outside of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retraining a model from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 20 #number of epochs to avoid overtraining\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_nod = model.fit(x_train, y_train, epochs=e, batch_size=512, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_nod\n",
    "history_dict = history.history\n",
    "h2 = history_nod.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc2 = h2[\"accuracy\"]\n",
    "val_acc2 = h2[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc2, \"bo\", label=\"Model:DropOut Training acc\")\n",
    "plt.plot(epochs, val_acc2, \"b\", label=\"Model:DropOut Validation acc\")\n",
    "plt.plot(epochs, acc, \"rx\", label=\"Model:Orig Training acc\")\n",
    "plt.plot(epochs, val_acc, \"r-.\", label=\"Model:Orig Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further experiments\n",
    "\n",
    "*   Try setting dropout rate to values (0, 1)\n",
    "*   You used two representation layers before the final classification layer. Try using one or three representation layers, and see how doing so affects validation and test accuracy.\n",
    "*   Try using callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Text preprocessing\n",
    "    - Encoding/Decoding data to work with a neural network\n",
    "\n",
    "- Monitoring for overfitting\n",
    "    - Number of epochs to train to avoid Validation testing performance loss\n",
    "    - Early stopping and other callbacks\n",
    "    \n",
    "- Improve Model Generalization\n",
    "    - Regularization via Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work\n",
    "\n",
    "What else we can do?\n",
    "- How to feature-ize text data / n-grams\n",
    "- Sequence modeling with CNN vs RNN/LSTM, transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of chapter04_getting-started-with-neural-networks.i",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}